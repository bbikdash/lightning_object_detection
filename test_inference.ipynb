{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "from lit_yolox import LitYOLOX\n",
    "\n",
    "def write2mp4(path, frames, fps=10):\n",
    "    writer = imageio.get_writer(path, fps=fps)\n",
    "\n",
    "    for f in frames:\n",
    "        writer.append_data(f)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images that simulate a ground vehicle view\n",
    "image1 = cv2.imread(\"/home/bbikdash/Development/1_object_detection/eotacs_demo_april_2023_car_detection/test_data/downward_vio_bin/image_raw/1332343552747.png\", 1)\n",
    "image2 = cv2.imread(\"/home/bbikdash/Development/1_object_detection/eotacs_demo_april_2023_car_detection/test_data/downward_vio_bin/image_raw/895651153410.png\", 1)\n",
    "image3 = cv2.imread(\"/home/bbikdash/Development/1_object_detection/eotacs_demo_april_2023_car_detection/test_data/downward_vio_bin/image_raw/887585870861.png\", 1)\n",
    "image4 = cv2.imread(\"/home/bbikdash/Development/1_object_detection/eotacs_demo_april_2023_car_detection/test_data/downward_vio_bin/image_raw/1097816758502.png\", 1)\n",
    "image5 = cv2.imread(\"/home/bbikdash/Development/1_object_detection/eotacs_demo_april_2023_car_detection/test_data/downward_vio_bin/image_raw/1219329453064.png\", 1)\n",
    "\n",
    "image6 = cv2.imread(\"/mnt/data/Datasets/KEF_Vesper_Vehicle_Evaluation_Datasets/demo/187719784054.png\", 1)\n",
    "image7 = cv2.imread(\"/mnt/data/Datasets/KEF_Vesper_Vehicle_Evaluation_Datasets/demo/190795563947.png\", 1)\n",
    "image8 = cv2.imread(\"/mnt/data/Datasets/KEF_Vesper_Vehicle_Evaluation_Datasets/demo/236421139624.png\", 1)\n",
    "image9 = cv2.imread(\"/mnt/data/Datasets/KEF_Vesper_Vehicle_Evaluation_Datasets/demo/187853094730.png\", 1)\n",
    "\n",
    "\n",
    "# image10 = cv2.imread(\"/mnt/data/Datasets/UAV-Vehicle-Detection-Dataset/data/train/DJI-00760-00022.jpg\", 1)\n",
    "image11 = cv2.imread(\"/mnt/data/Datasets/UAV-Vehicle-Detection-Dataset/data/val/MOS86.png\", 1)\n",
    "\n",
    "\n",
    "image12 = cv2.imread(\"/mnt/data/Datasets/Building_Detection/eo/ETG/11_17_08/11_17_08_img95.png\")\n",
    "image13 = cv2.imread(\"/mnt/data/Datasets/Building_Detection/eo/ETG/12_45_35/12_45_35_img157.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Lightning Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model = LitYOLOX.load_from_checkpoint(\"./logs/yolox_s_eo_building/inria_pretrain/version_4/checkpoints/epoch=85-step=26230.ckpt\")\n",
    "model.to(device)\n",
    "logger.info(model.device)\n",
    "\n",
    "val_input_size = [640, 640]\n",
    "transforms = A.Compose([\n",
    "    # Geometric transformations\n",
    "    A.Resize(val_input_size[0], val_input_size[1], always_apply=True), \n",
    "    # A.SmallestMaxSize(val_input_size[0], always_apply=True),\n",
    "    # A.CenterCrop(val_input_size[0], val_input_size[1], always_apply=True),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "image = image13\n",
    "\n",
    "pred = model.inference(image, 0.25, 0.5, transforms)\n",
    "\n",
    "# Visualize original image, inference without post-processing, and image with post-processing\n",
    "figure, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 10), frameon=True, layout=\"tight\", dpi=300)\n",
    "ax[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) ; ax[0].set_title(\"Original Image\") ; ax[0].set_axis_off()\n",
    "ax[1].imshow(cv2.cvtColor(pred, cv2.COLOR_BGR2RGB)) ; ax[1].set_title(\"Bounding Box Prediction\") ; ax[1].set_axis_off()\n",
    "\n",
    "# Save image as an asset for README\n",
    "# Bounds of the plotting function\n",
    "# h,w,c = image.shape\n",
    "# horizontal_bounds = [-w//2, w//2]\n",
    "# vertical_bounds = [-h//2, h//2]\n",
    "# figure, ax = plt.subplots(figsize=(10,10), frameon=False, layout='tight')\n",
    "# ax.imshow(cv2.cvtColor(pred, cv2.COLOR_BGR2RGB), extent=[horizontal_bounds[0], horizontal_bounds[1], vertical_bounds[0], vertical_bounds[1]])\n",
    "# ax.set_xlim(horizontal_bounds[0], horizontal_bounds[1])\n",
    "# ax.set_ylim(vertical_bounds[0], vertical_bounds[1])\n",
    "# ax.set_axis_off()\n",
    "# plt.gca()\n",
    "# plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, \n",
    "#             hspace = 0, wspace = 0)\n",
    "# plt.margins(0,0)\n",
    "# plt.savefig(\"./car_detections_asset.png\", bbox_inches='tight', pad_inches=0) # After years: https://stackoverflow.com/questions/11837979/removing-white-space-around-a-saved-image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOX ONNX Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define preprocess parameters\n",
    "mean = np.array([0.485, 0.456, 0.406]) * 255.0\n",
    "scale_factor = 1 / (255.0)\n",
    "std = [0.229, 0.224, 0.225]\n",
    "height, width = 640, 640\n",
    "scale = np.array([[width, height, width, height]])\n",
    "CLASS_LABELS = ['vehicle']\n",
    "CLASS_IND = np.arange(1)\n",
    "class_colors = [(1.0, 0.0, 0.0) for _ in range(5)]\n",
    "\n",
    "# num_classes = 1\n",
    "# confidence_thresh = 0.15\n",
    "# overlap_thresh = 0.1\n",
    "# iou_thresh = 0.75\n",
    "# nms_threshold = 0.5\n",
    "GRID = None\n",
    "def _pre_process(image: np.ndarray) -> np.ndarray:\n",
    "    # Accepts raw RGB image loaded from opencv. Dim: HxWx3\n",
    "    # prepare input blob to fit the model input:\n",
    "    # 1. subtract mean\n",
    "    # 2. scale to set pixel values from 0 to 1\n",
    "    input_blob = cv2.dnn.blobFromImage(\n",
    "        image=image,\n",
    "        # scalefactor=scale_factor,\n",
    "        size=(height, width),  # img target size\n",
    "        # mean=mean,\n",
    "        swapRB=False,  # BGR -> RGB\n",
    "        # crop=True  # center crop\n",
    "    )\n",
    "    # 3. divide by std\n",
    "    # input_blob[0] /= np.asarray(std, dtype=np.float32).reshape(3, 1, 1)\n",
    "    return input_blob\n",
    "\n",
    "_TORCH_VER = [int(x) for x in torch.__version__.split(\".\")[:2]]\n",
    "def meshgrid(*tensors):\n",
    "    if _TORCH_VER >= [1, 10]:\n",
    "        return torch.meshgrid(*tensors, indexing=\"ij\")\n",
    "    else:\n",
    "        return torch.meshgrid(*tensors)\n",
    "    \n",
    "def decode_outputs(outputs, dtype):\n",
    "    # This is determined by the input size of the image.\n",
    "    hw =  [(80, 80),\n",
    "           (40, 40),\n",
    "           (20, 20),\n",
    "    ]   # I don't know what this is. It was created with this: hw = [x.shape[-2:] for x in outputs]\n",
    "\n",
    "    arch_strides = [8, 16, 32] # See yolox architectures for the appropriate strides\n",
    "\n",
    "    grids = []\n",
    "    strides = []\n",
    "    for (hsize, wsize), stride in zip(hw, arch_strides):\n",
    "        yv, xv = meshgrid([torch.arange(hsize), torch.arange(wsize)])\n",
    "        grid = torch.stack((xv, yv), 2).view(1, -1, 2)\n",
    "        grids.append(grid)\n",
    "        shape = grid.shape[:2]\n",
    "        strides.append(torch.full((*shape, 1), stride))\n",
    "\n",
    "    grids = torch.cat(grids, dim=1).type(dtype)\n",
    "    strides = torch.cat(strides, dim=1).type(dtype)\n",
    "    \n",
    "    np.savetxt('./grids.txt', grids[0].detach().cpu().numpy().astype(np.int32))\n",
    "\n",
    "\n",
    "    print(grids[0, 8000:8010])\n",
    "    print(strides[0, 8000:8010])\n",
    "\n",
    "    outputs = torch.cat([\n",
    "        (outputs[..., 0:2] + grids) * strides,\n",
    "        torch.exp(outputs[..., 2:4]) * strides,\n",
    "        outputs[..., 4:]\n",
    "    ], dim=-1)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def postprocess(prediction, num_classes, conf_thre=0.7, nms_thre=0.45, class_agnostic=False):\n",
    "    # box_corner = prediction.new(prediction.shape)\n",
    "    box_corner = torch.zeros_like(prediction)\n",
    "    box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n",
    "    box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
    "    box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
    "    box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
    "    prediction[:, :, :4] = box_corner[:, :, :4]\n",
    "\n",
    "    output = [None for _ in range(len(prediction))]\n",
    "    for i, image_pred in enumerate(prediction):\n",
    "\n",
    "        # If none are remaining => process next image\n",
    "        if not image_pred.size(0):\n",
    "            continue\n",
    "        # Get score and class with highest confidence\n",
    "        class_conf, class_pred = torch.max(image_pred[:, 5: 5 + num_classes], 1, keepdim=True)\n",
    "\n",
    "        conf_mask = (image_pred[:, 4] * class_conf.squeeze() >= conf_thre).squeeze()\n",
    "        # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)\n",
    "        detections = torch.cat((image_pred[:, :5], class_conf, class_pred.float()), 1)\n",
    "        detections = detections[conf_mask]\n",
    "        if not detections.size(0):\n",
    "            continue\n",
    "\n",
    "        if class_agnostic:\n",
    "            nms_out_index = torchvision.ops.nms(\n",
    "                detections[:, :4],\n",
    "                detections[:, 4] * detections[:, 5],\n",
    "                nms_thre,\n",
    "            )\n",
    "        else:\n",
    "            nms_out_index = torchvision.ops.batched_nms(\n",
    "                detections[:, :4],\n",
    "                detections[:, 4] * detections[:, 5],\n",
    "                detections[:, 6],\n",
    "                nms_thre,\n",
    "            )\n",
    "\n",
    "        detections = detections[nms_out_index]\n",
    "        if output[i] is None:\n",
    "            output[i] = detections\n",
    "        else:\n",
    "            output[i] = torch.cat((output[i], detections))\n",
    "\n",
    "    return output\n",
    "\n",
    "    \n",
    "def vis(img, boxes, scores, cls_ids, conf=0.5, class_names: List[str] = None):\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        box = boxes[i]\n",
    "        cls_id = int(cls_ids[i])\n",
    "        score = scores[i]\n",
    "        if score < conf:\n",
    "            continue\n",
    "        x0 = int(box[0])\n",
    "        y0 = int(box[1])\n",
    "        x1 = int(box[2])\n",
    "        y1 = int(box[3])\n",
    "\n",
    "        color = (_COLORS[cls_id] * 255).astype(np.uint8).tolist()\n",
    "        text = '{}:{:.1f}%'.format(class_names[cls_id], score * 100)\n",
    "        txt_color = (0, 0, 0) if np.mean(_COLORS[cls_id]) > 0.5 else (255, 255, 255)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "        txt_size = cv2.getTextSize(text, font, 0.4, 1)[0]\n",
    "        cv2.rectangle(img, (x0, y0), (x1, y1), color, 2)\n",
    "\n",
    "        txt_bk_color = (_COLORS[cls_id] * 255 * 0.7).astype(np.uint8).tolist()\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            (x0, y0 + 1),\n",
    "            (x0 + txt_size[0] + 1, y0 + int(1.5*txt_size[1])),\n",
    "            txt_bk_color,\n",
    "            -1\n",
    "        )\n",
    "        cv2.putText(img, text, (x0, y0 + txt_size[1]), font, 0.4, txt_color, thickness=1)\n",
    "\n",
    "    return img\n",
    "\n",
    "_COLORS = np.array(\n",
    "    [\n",
    "        0.000, 0.447, 0.741,\n",
    "        0.850, 0.325, 0.098,\n",
    "        0.929, 0.694, 0.125,\n",
    "        0.494, 0.184, 0.556,\n",
    "        0.466, 0.674, 0.188,\n",
    "        0.301, 0.745, 0.933,\n",
    "        0.635, 0.078, 0.184,\n",
    "        0.300, 0.300, 0.300,\n",
    "        0.600, 0.600, 0.600,\n",
    "        1.000, 0.000, 0.000,\n",
    "        1.000, 0.500, 0.000,\n",
    "        0.749, 0.749, 0.000,\n",
    "        0.000, 1.000, 0.000,\n",
    "        0.000, 0.000, 1.000,\n",
    "        0.667, 0.000, 1.000,\n",
    "        0.333, 0.333, 0.000,\n",
    "        0.333, 0.667, 0.000,\n",
    "        0.333, 1.000, 0.000,\n",
    "        0.667, 0.333, 0.000,\n",
    "        0.667, 0.667, 0.000,\n",
    "        0.667, 1.000, 0.000,\n",
    "        1.000, 0.333, 0.000,\n",
    "        1.000, 0.667, 0.000,\n",
    "        1.000, 1.000, 0.000,\n",
    "        0.000, 0.333, 0.500,\n",
    "        0.000, 0.667, 0.500,\n",
    "        0.000, 1.000, 0.500,\n",
    "        0.333, 0.000, 0.500,\n",
    "        0.333, 0.333, 0.500,\n",
    "        0.333, 0.667, 0.500,\n",
    "        0.333, 1.000, 0.500,\n",
    "        0.667, 0.000, 0.500,\n",
    "        0.667, 0.333, 0.500,\n",
    "        0.667, 0.667, 0.500,\n",
    "        0.667, 1.000, 0.500,\n",
    "        1.000, 0.000, 0.500,\n",
    "        1.000, 0.333, 0.500,\n",
    "        1.000, 0.667, 0.500,\n",
    "        1.000, 1.000, 0.500,\n",
    "        0.000, 0.333, 1.000,\n",
    "        0.000, 0.667, 1.000,\n",
    "        0.000, 1.000, 1.000,\n",
    "        0.333, 0.000, 1.000,\n",
    "        0.333, 0.333, 1.000,\n",
    "        0.333, 0.667, 1.000,\n",
    "        0.333, 1.000, 1.000,\n",
    "        0.667, 0.000, 1.000,\n",
    "        0.667, 0.333, 1.000,\n",
    "        0.667, 0.667, 1.000,\n",
    "        0.667, 1.000, 1.000,\n",
    "        1.000, 0.000, 1.000,\n",
    "        1.000, 0.333, 1.000,\n",
    "        1.000, 0.667, 1.000,\n",
    "        0.333, 0.000, 0.000,\n",
    "        0.500, 0.000, 0.000,\n",
    "        0.667, 0.000, 0.000,\n",
    "        0.833, 0.000, 0.000,\n",
    "        1.000, 0.000, 0.000,\n",
    "        0.000, 0.167, 0.000,\n",
    "        0.000, 0.333, 0.000,\n",
    "        0.000, 0.500, 0.000,\n",
    "        0.000, 0.667, 0.000,\n",
    "        0.000, 0.833, 0.000,\n",
    "        0.000, 1.000, 0.000,\n",
    "        0.000, 0.000, 0.167,\n",
    "        0.000, 0.000, 0.333,\n",
    "        0.000, 0.000, 0.500,\n",
    "        0.000, 0.000, 0.667,\n",
    "        0.000, 0.000, 0.833,\n",
    "        0.000, 0.000, 1.000,\n",
    "        0.000, 0.000, 0.000,\n",
    "        0.143, 0.143, 0.143,\n",
    "        0.286, 0.286, 0.286,\n",
    "        0.429, 0.429, 0.429,\n",
    "        0.571, 0.571, 0.571,\n",
    "        0.714, 0.714, 0.714,\n",
    "        0.857, 0.857, 0.857,\n",
    "        0.000, 0.447, 0.741,\n",
    "        0.314, 0.717, 0.741,\n",
    "        0.50, 0.5, 0\n",
    "    ]\n",
    ").astype(np.float32).reshape(-1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image pre-processing transformations\n",
    "# Load detection model\n",
    "model_path = \"/home/bbikdash/Development/1_object_detection/kef_yolox/weights/yolox_s_eo_building.onnx\"\n",
    "opencv_net = cv2.dnn.readNetFromONNX(model_path)\n",
    "\n",
    "image = image13\n",
    "logger.info(f\"Image Shape: {image.shape}\")\n",
    "\n",
    "# Measure pre-processing time\n",
    "start_preproc = time.time()\n",
    "network_input = _pre_process(image)\n",
    "logger.info(f\"Network Input Shape: {network_input.shape}\")\n",
    "end_preproc = time.time()\n",
    "\n",
    "# Measure inference time\n",
    "start_inference = time.time()\n",
    "\n",
    "# set OpenCV DNN input\n",
    "opencv_net.setInput(network_input)\n",
    "# OpenCV DNN inference\n",
    "output = opencv_net.forward()\n",
    "logger.info(f\"Network Output Shape: {output.shape}\")\n",
    "\n",
    "decode_outputs = decode_outputs(torch.from_numpy(output), dtype=torch.float)\n",
    "detections = postprocess(decode_outputs, num_classes=1, conf_thre=0.25, class_agnostic=True)[0]\n",
    "end_inference = time.time()\n",
    "\n",
    "\n",
    "start_draw_time = time.time()\n",
    "background = network_input[0].transpose(1,2,0).copy()\n",
    "visualization_img = network_input[0].transpose(1,2,0).copy()\n",
    "\n",
    "if detections != None:\n",
    "      detections = detections.detach().cpu().numpy()\n",
    "      bboxes = detections[:, 0:4]\n",
    "      cls = detections[:, 6]\n",
    "      scores = detections[:, 4] * detections[:, 5]\n",
    "      print(bboxes)\n",
    "      visualization_img = vis(visualization_img, bboxes, scores, cls, 0.1, ['vehicle'])\n",
    "\n",
    "      # Visualize bounding boxes in [0,255] range\n",
    "end_draw_time = time.time()\n",
    "\n",
    "# Normalize images for visualization\n",
    "visualization_img = (visualization_img - visualization_img.min()) / (visualization_img.max() - visualization_img.min())\n",
    "background = (background - background.min()) / (background.max() - background.min())\n",
    "\n",
    "# print(\"OpenCV DNN prediction: \\n\")\n",
    "# print(\"\\toutput: \", output.shape)\n",
    "# print(\"\\tbboxes: \", bboxes.shape)\n",
    "# print(\"\\tscores: \", scores.shape)\n",
    "# print(\"\\tclass_indices: \", class_indices.shape)\n",
    "\n",
    "# Visualize original image, inference without post-processing, and image with post-processing\n",
    "figure, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 10), frameon=True, layout=\"tight\", dpi=300)\n",
    "ax[0].imshow(cv2.cvtColor(background, cv2.COLOR_BGR2RGB)) ; ax[0].set_title(\"Network Input\") ; ax[0].set_axis_off()\n",
    "ax[1].imshow(cv2.cvtColor(visualization_img, cv2.COLOR_BGR2RGB)) ; ax[1].set_title(\"Prediction\") ; ax[1].set_axis_off()\n",
    "# ax[2].imshow(mask_proc) ; ax[2].set_title(\"With Preprocessing\") ; ax[2].set_axis_off()\n",
    "\n",
    "print(f\"Pre-Processing time: {end_preproc-start_preproc}\\n\"\n",
    "      f\"Inference time: {end_inference-start_inference}\\n\"\n",
    "      # f\"Post-Processing time: {end_postproc-start_postproc}\\n\"\n",
    "      f\"Drawing time: {end_draw_time-start_draw_time}\\n\"\n",
    "      f\"\\tPipeline Time: {end_draw_time-start_preproc}\\n\")\n",
    "      # f\"{with_post_proc.shape}, {np.max(with_post_proc)}, {np.min(with_post_proc)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b43a8734535082539493a11dd2746561a737ba4427a42e5cd1c5a5ff43b266b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
